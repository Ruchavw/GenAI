{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgC2jH7sxsl7"
   },
   "source": [
    "### Project: 25. Dialogue Autocomplete\n",
    "\n",
    "**Goal:** Assist screenwriters by suggesting the next line of dialogue.\n",
    "\n",
    "**Model Used:** GPT-2\n",
    "\n",
    "**Reason for Model Choice:**  GPT-2 is a decoder-only model trained for text generation and dialogue continuation.\n",
    "\n",
    "**Outcome:**  \n",
    "The system successfully generates mostly meaningful and context-aware dialogue suggestions based on previous conversation lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fefd7beb0748528426be5a8578ba5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4c603ebe2b44e9a71dcb9c3145c0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed71ba18f2c7464896223e6cf45cbea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551eba68d6e4c11b5d3eafef90e444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53118f637dce479db4909ab1955fc0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f96785431646abb18078ec2ef34bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c057470e754423aba7a7e1dc8f8018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "dialogue_generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_GKEKoWyPi5"
   },
   "source": [
    "Example 1: Sarah and Kiara's conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_context = \"\"\"\n",
    "Kiara: I don't think this is a good idea.\n",
    "Sarah: Do you have a better idea?\n",
    "Kiara: No. But, what if everything goes wrong?\n",
    "Sarah: ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kiara: I don't think this is a good idea.\n",
      "Sarah: Do you have a better idea?\n",
      "Kiara: No. But, what if everything goes wrong?\n",
      "Sarah: ...\n",
      "Kiara: I think...\n",
      "Kiara: That is a great idea.\n",
      "Sarah: Good idea.\n",
      "Kiara: I think the best thing for you is for us to go out and do what we do best.\n",
      "Sarah: Go out and do what we do best now.\n",
      "Sarah: And what would you do with that?\n",
      "Kiara: I would go out into the world and do what I do best.\n",
      "Sarah: I think you can do it.\n",
      "Kiara: I think you can do it better than I can.\n",
      "Sarah: And, for the first time in my life, I hope I can do it better than you.\n",
      "Kiara: But, I'm not sure I can, and I'm not sure I can keep doing what I would do better if I didn't have to.\n",
      "Sarah: And, for the first time in my life, I hope I can do it better than you.\n",
      "Kiara: Now, I think you're right.\n",
      "Sarah: But, I don't think you can keep doing what you would do better if you didn't have to.\n",
      "Kiara: I think you can do it better than you. And, for the\n"
     ]
    }
   ],
   "source": [
    "output = dialogue_generator(dialogue_context, max_length=100, num_return_sequences=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_YcLHDCyZkQ"
   },
   "source": [
    "Example 2: A detective interrogates a suspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=90) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detective: Where were you last night?\n",
      "Suspect: I was at home.\n",
      "Detective: Can anyone confirm that?\n",
      "Suspect: My wife. She was with me the entire evening.\n",
      "Detective: ...\n",
      "You know her name?\n",
      "Suspect: My wife.\n",
      "Detective: What was that?\n",
      "Suspect: It was a bad night.\n",
      "Detective: What is that?\n",
      "Suspect: I was running out of money.\n",
      "Detective: So there will be no one to talk to?\n",
      "Suspect: Nope.\n",
      "Detective: Can you believe it?\n",
      "Suspect: No.\n",
      "Detective: How many people are there?\n",
      "Suspect: I think we're looking at more than three million.\n",
      "Detective: Are you sure you're not the only one that's been arrested?\n",
      "Suspect: No.\n",
      "Detective: You have two other leads already?\n",
      "Suspect: No.\n",
      "Detective: You have three other leads?\n",
      "Suspect: No.\n",
      "Detective: The other leads were all about your brother's arrest.\n",
      "Suspect: Yeah.\n",
      "Detective: Well, I think that leads to the second one. If you have any other leads, can you tell me who they are?\n",
      "Suspect: I believe we're looking at a girl and a boy, and they've got something in their pants.\n",
      "Detective: And they're all black?\n",
      "Suspect:\n"
     ]
    }
   ],
   "source": [
    "dialogue_context = \"\"\"\n",
    "Detective: Where were you last night?\n",
    "Suspect: I was at home.\n",
    "Detective: Can anyone confirm that?\n",
    "Suspect: My wife. She was with me the entire evening.\n",
    "Detective: ...\n",
    "\"\"\"\n",
    "\n",
    "output = dialogue_generator(dialogue_context, max_length=90, num_return_sequences=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4ji8e2Zv8NW"
   },
   "source": [
    "### Working and Explanation\n",
    "\n",
    "- GPT-2 is a decoder-only model trained to predict the next word\n",
    "- Previous dialogue lines are given as input\n",
    "- The model autocompletes the next line naturally\n",
    "- No training is required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNu8urpgxZN5"
   },
   "source": [
    "### Architectural Justification\n",
    "\n",
    "- GPT-2 is a text generation model\n",
    "- It predicts the next token based on previous tokens\n",
    "- Encoder models like BERT cannot generate dialogue\n",
    "- This makes GPT-2 suitable for dialogue autocomplete\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
