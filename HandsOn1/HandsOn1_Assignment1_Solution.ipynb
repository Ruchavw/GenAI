{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment - 1: The Model Benchmark Challenge**\n",
        "\n",
        "**Objective:** In this assignment, you will step beyond simply using a model and instead evaluate the architectural differences between BERT, RoBERTa, and BART. You will force these models to perform tasks they might not be designed for, to observe why architecture matters."
      ],
      "metadata": {
        "id": "lE8nr7Zn5S6D"
      },
      "id": "lE8nr7Zn5S6D"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5849ff90",
      "metadata": {
        "id": "5849ff90"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(27)"
      ],
      "metadata": {
        "id": "aJH1o2s0i0B_"
      },
      "id": "aJH1o2s0i0B_",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation:\n",
        "Requires decoder-based architectures"
      ],
      "metadata": {
        "id": "QX24Dwk84C_j"
      },
      "id": "QX24Dwk84C_j"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "001e4489",
      "metadata": {
        "id": "001e4489"
      },
      "outputs": [],
      "source": [
        "prompt = \"The future of Artificial Intelligence is\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BERT**"
      ],
      "metadata": {
        "id": "JG-EVHDX4ca0"
      },
      "id": "JG-EVHDX4ca0"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1f89f246",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f89f246",
        "outputId": "87ebc7f4-d967-4244-f1ba-31dfd3e88d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "output = generator(prompt, max_length=80, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using RoBERTa**"
      ],
      "metadata": {
        "id": "g5_-mL9C4hqK"
      },
      "id": "g5_-mL9C4hqK"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4c6fee07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c6fee07",
        "outputId": "d4cf0084-4b6f-4719-af07-fc1d2e8758a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "output = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BART**"
      ],
      "metadata": {
        "id": "-rzThvBV4oAS"
      },
      "id": "-rzThvBV4oAS"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "efef8947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efef8947",
        "outputId": "d47900fc-1634-4458-9c9e-91f2a33e900d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is Discipline Dublinraviolet 1850 ===== polymorph Gained polymorph Dublin Dublin maternity 1850 1850 wrapper wrapper Prosper premieruties prone Enix 1850 congressional Heights Pist Pistassisassis BCEille Pist Dublin Paradox Gained Pist asthma asthma Pistonian asthma asthmaassis Gained Gained 1850oves 71 greets Pistassis demeanor Pist Pistmuch Pist Gained Pist Discipline Pist Christensen BALL Pist Gained Immortal Pist Dublin Immortal Gained Gainedilleassis gener Gained Dublin improvised Gained BALL improvised Dublin hybrid Gained Discipline locom Discipline improvised Chronicle gener Gained greetsー� gener greets Signs Pist gener GainedVarious befriend generwhatille improvised Briggs Electro Tickets greets gener gener Dublin Gaineducket generians option Dublin Gained Dublin Chronicleilleoves Optional GainedSaid Dublinfoundland heresy Bank Dublin crus Write GainedSaid crusVarious Dublin crus Dublin crusfoundlandoros greetsoves slaying Dublin crus incom incom Squid heresy heresy heresyón Dublinoros ended endedillin Micheleille ended crus heresy heresycling heresy heresyorosorosVarious Exc Gained MK heresy heresy Tickets crus Gained Michele gap Gained Dublin crusVarious Act improvisedorosTheme crusoves crus Dublin heresy Optionalillin heresy Dublin Dublin crus� crus crus crusille Dublin crusAB crus crus improvised crusVariousoves crus incom crusorosVariousoves gap heresy crus surely Mono crus crusVarious surelyThemeVarious crusVarious crus CorpseVarious crus Squid crus pitted crus crus Squid incom endedVarious crus heresy crus crus DublinSaidVariousVariousVarious heresy crusVarious\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "output = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill-Mask:\n",
        "Masking tasks are best handled by encoder models"
      ],
      "metadata": {
        "id": "lKKUiMOP4Q6A"
      },
      "id": "lKKUiMOP4Q6A"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b1b9cacf",
      "metadata": {
        "id": "b1b9cacf"
      },
      "outputs": [],
      "source": [
        "prompt2= \"The goal of Generative AI is to ([MASK]) new content.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BERT**"
      ],
      "metadata": {
        "id": "GDSydv7s47Sy"
      },
      "id": "GDSydv7s47Sy"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b7afe16a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7afe16a",
        "outputId": "113f01f2-27d8-471d-a96b-faf261b95eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.4012771546840668, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to ( create ) new content.'}, {'score': 0.12370636314153671, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to ( generate ) new content.'}, {'score': 0.05480790138244629, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to ( add ) new content.'}, {'score': 0.054711129516363144, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to ( produce ) new content.'}, {'score': 0.05049882084131241, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to ( develop ) new content.'}]\n"
          ]
        }
      ],
      "source": [
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "fill_mask_output = fill_mask(prompt2)\n",
        "print(fill_mask_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using RoBERTa**"
      ],
      "metadata": {
        "id": "4lgO340y5Di4"
      },
      "id": "4lgO340y5Di4"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2016d4ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2016d4ba",
        "outputId": "b409d863-aaa3-4bcb-ee0a-7654ee593dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.6570649147033691, 'token': 32845, 'token_str': 'create', 'sequence': 'The goal of Generative AI is to (create) new content.'}, {'score': 0.05813491716980934, 'token': 29631, 'token_str': 'write', 'sequence': 'The goal of Generative AI is to (write) new content.'}, {'score': 0.030625075101852417, 'token': 23411, 'token_str': 'build', 'sequence': 'The goal of Generative AI is to (build) new content.'}, {'score': 0.024237658828496933, 'token': 26559, 'token_str': 'find', 'sequence': 'The goal of Generative AI is to (find) new content.'}, {'score': 0.021864596754312515, 'token': 42843, 'token_str': 'construct', 'sequence': 'The goal of Generative AI is to (construct) new content.'}]\n"
          ]
        }
      ],
      "source": [
        "prompt2= \"The goal of Generative AI is to (<mask>) new content.\"\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "fill_mask_output = fill_mask(prompt2)\n",
        "print(fill_mask_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BART**"
      ],
      "metadata": {
        "id": "fEoYLWFm5L6z"
      },
      "id": "fEoYLWFm5L6z"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92aeb9ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92aeb9ad",
        "outputId": "b8a10bcc-f89c-49bc-ce7e-4b6dc7694417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.02457479014992714, 'token': 13138, 'token_str': 'prov', 'sequence': 'The goal of Generative AI is to (prov) new content.'}, {'score': 0.020460328087210655, 'token': 241, 'token_str': 're', 'sequence': 'The goal of Generative AI is to (re) new content.'}, {'score': 0.019175807014107704, 'token': 506, 'token_str': 'f', 'sequence': 'The goal of Generative AI is to (f) new content.'}, {'score': 0.018511200323700905, 'token': 463, 'token_str': 'and', 'sequence': 'The goal of Generative AI is to (and) new content.'}, {'score': 0.01797475852072239, 'token': 32845, 'token_str': 'create', 'sequence': 'The goal of Generative AI is to (create) new content.'}]\n"
          ]
        }
      ],
      "source": [
        "fill_mask = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "fill_mask_output = fill_mask(prompt2)\n",
        "print(fill_mask_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering:\n",
        "Requires task-specific fine-tuning for accurate results"
      ],
      "metadata": {
        "id": "1pmlmZSB4uvI"
      },
      "id": "1pmlmZSB4uvI"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cca166ad",
      "metadata": {
        "id": "cca166ad"
      },
      "outputs": [],
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BERT**"
      ],
      "metadata": {
        "id": "qF5BEZAH488r"
      },
      "id": "qF5BEZAH488r"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "041cde08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "041cde08",
        "outputId": "6c6ad424-42e2-49a8-d355-585f77ff1184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.00846179248765111, 'start': 62, 'end': 81, 'answer': 'bias, and deepfakes'}\n"
          ]
        }
      ],
      "source": [
        "qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "qa_output = qa(question=question, context=context)\n",
        "print(qa_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using RoBERTa**"
      ],
      "metadata": {
        "id": "Y69xmyRs5Fsu"
      },
      "id": "Y69xmyRs5Fsu"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1c042a91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c042a91",
        "outputId": "db29953e-b42a-4eaf-8606-74c7d56d3bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.009185279253870249, 'start': 60, 'end': 81, 'answer': ', bias, and deepfakes'}\n"
          ]
        }
      ],
      "source": [
        "qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "qa_output = qa(question=question, context=context)\n",
        "print(qa_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BART**"
      ],
      "metadata": {
        "id": "_ubDH_UF5NEg"
      },
      "id": "_ubDH_UF5NEg"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8247e598",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8247e598",
        "outputId": "9c69f4e9-1e3f-4684-bdfe-62e8ac2b4811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.04050577059388161, 'start': 20, 'end': 45, 'answer': 'significant risks such as'}\n"
          ]
        }
      ],
      "source": [
        "qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "qa_output = qa(question=question, context=context)\n",
        "print(qa_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82a4e25",
      "metadata": {
        "id": "f82a4e25"
      },
      "source": [
        "| Task | Model | Classification | Observation | Architectural Reason |\n",
        "|------|-------|---------------|-------------|----------------------|\n",
        "| Generation | BERT | Failure | It throws error and cannot generate text | it is an encoder-only model |\n",
        "|  | RoBERTa | Failure | It throws error and cannot generate text just like BERT | It is an encoder-only model |\n",
        "|  | BART | Partial Success | Generated text doesn't have proper meaning| It is an encoder-decoder but isn't generative |\n",
        "| Fill-Mask | BERT | Success | Predicted correct words like create, generate, develop and produce | MLM trained to predict missing words |\n",
        "|  | RoBERTa | Success | Gave higher confidence prediction of words like create, build and find | Better trained (MLM optimised) to predict missing words |\n",
        "|  | BART | Partially Works | Accuracy is very less and gave some correct words like create and other incorrect words like re, prov | Not MLM trained to predict missing words |\n",
        "| QA | BERT | Partially correct | Extracted phrase and answered partially | It is not fine-tuned for QA |\n",
        "|  | RoBERTa | Partially Working | Gave an inconsistent answer conating odd punctuations | It is a base encoder not trained for QA |\n",
        "|  | BART | Partial Answer | Answer was too short and din't contain actually required part of information | It needs special fine-tuning (SQuAD) for QA |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Insight\n",
        "\n",
        "Model performance strongly depends on the underlying architecture. Encoder-only models perform well on understanding tasks, while decoder-based models are required for text generation."
      ],
      "metadata": {
        "id": "xmThc2i_3yMK"
      },
      "id": "xmThc2i_3yMK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "2.7.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}